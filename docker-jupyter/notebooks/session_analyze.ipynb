{
  "metadata": {
    "name": "example",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsc"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport sys\nsys.version_info"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport json\nraw \u003d \u0027[{\"name\": \"grab\"}, {\"name\": \"graby\"}, {\"name\": \"new grab\"}]\u0027\n\nprint(json.loads(raw))"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n#User Analytics Data 다운로드\nwget --no-check-certificate -r \u0027https://docs.google.com/uc?export\u003ddownload\u0026id\u003d1MdN9jWZfiw1gFIfCzJlm8t0b2ooBYLSP\u0027 -O raw_data.zip\nunzip raw_data.zip\nhead /zeppelin/session_timestamp.csv"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#df로 import하기\ndf_session_ts \u003d spark.read.option(\"header\", \"true\").csv(\u0027/raw_data/session_timestamp.csv\u0027)\ndf_session_tx \u003d spark.read.option(\"header\", \"true\").csv(\u0027/raw_data/session_transaction.csv\u0027)\ndf_channel \u003d spark.read.option(\"header\", \"true\").csv(\u0027/raw_data/channel.csv\u0027)\ndf_usc \u003d spark.read.option(\"header\", \"true\").csv(\u0027/raw_data/user_session_channel.csv\u0027)\n\n\n# 전체 dataframe 출력\n# df_session_ts.show()|\n\n# explain으로 실행계획 확인하기\n# df_session_ts.select(\"ts\").explain()\n\ndf_usc.limit(1).show()\ndf_session_ts.limit(1).show()\ndf_session_tx.limit(1).show()\ndf_channel.limit(1).show()\n\n#SQL에 사용할 TempView를 만들기\ndf_usc.createOrReplaceTempView(\"usc\")\ndf_session_ts.createOrReplaceTempView(\"st\")\ndf_session_tx.createOrReplaceTempView(\"tx\")\ndf_channel.createOrReplaceTempView(\"ch\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#1. 특정 컬럼만 출력하기\n# df_session_ts.select(\u0027sessionid\u0027).show()\n\n# from pyspark.sql.functions import max\n# df_session_tx.select(max(\"amount\")).take(1)\n\n#2. 특정 Row만 필터링하기\n# df_session_ts.filter(df[\u0027ts\u0027] \u003c \u00272019-05-01 01:00:00\u0027).show()\n\n#3. groupby하기 \n# df_session_ts.groupby(\"sessionid\").agg({\u0027ts\u0027: \u0027count\u0027}).show()\n\n#4. sort 해서 2개만 가져오기\n# desc하려면 모듈 import 필요\n# from pyspark.sql.functions import desc\n\n# df_session_ts.sort(\u0027ts\u0027).take(2)\n# df_session_ts.sort(desc(\u0027ts\u0027)).take(2)\n\n\n\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Monthly Active User 구하기\n\n# df_usc.show()\n# df_session_ts.show()\nfrom pyspark.sql.functions import count\ndf_session_ts.alias(\"st\").join(df_usc.alias(\"usc\"), df_session_ts.sessionid \u003d\u003d df_usc.sessionid, how\u003d\u0027left\u0027).groupby(\u0027st.ts\u0027).agg(count(\u0027usc.userid\u0027)).show()\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SQL로 Growth 지표 뽑아보기"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# MAU 구하기\nspark.sql(\"\"\"\nSELECT date_format(st.ts, \"yyyy-MM\") as month , count(usc.userid)\nFROM usc \nLEFT JOIN st on usc.sessionid \u003d st.sessionid \nGROUP BY 1\nORDER BY 1\n\"\"\").show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Net Revenue가 가장 큰 UserId 10개 찾기\nspark.sql(\"\"\"\nSELECT usc.userid as uid, SUM(IF(tx.refunded \u003d false, tx.amount, 0)) as total\nFROM tx\nLEFT JOIN usc\nON tx.sessionid \u003d usc.sessionid\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 10\n\"\"\").show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#채녈 별 사용자 구하기\nspark.sql(\"\"\"\nSELECT ch.channelname, COUNT(usc.userid) as count\nFROM ch\nLEFT JOIN usc\nON ch.channelname \u003d usc.channel\nGROUP BY ch.channelname\nORDER BY 2 desc\n\"\"\").show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%sql\n\nSELECT distinct userid, FIRST_VALUE(channel) over (partition by userid order by ts rows between unbounded preceding and unbounded following) as first, LAST_VALUE(channel) over (partition by userid order by ts rows between unbounded preceding and unbounded following) as last\nFROM usc\nLEFT JOIN st\nON st.sessionid \u003d usc.sessionid\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    DATE_FORMAT(st.ts,\"YYYY-MM\") as month, \n    channel, \n    count(distinct usc.userid) as uniqueUsers,\n    count(amount) as paidUsers,\n    CONCAT(IF(count(amount) \u003d 0, 0,round((count(amount) / count(distinct usc.userid)) * 100,1)),\u0027%\u0027) as conversionRate,\n    SUM(amount) as growthRevenue,\n    SUM(IF(refunded \u003d FALSE ,amount, 0)) as netRevenue\nFROM ch\nLEFT JOIN usc\non ch.channelname \u003d usc.channel\nLEFT JOIN st\non st.sessionid \u003d usc.sessionid\nLEFT JOIN tx\non tx.sessionid \u003d usc.sessionid\nWHERE channel IS NOT NULL\nGROUP BY 1, 2\nORDER BY 1, 2\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import months_between, countDistinct, col\n\ncohorts \u003d spark.sql(\"\"\"\nSELECT \n    cohort_month,\n    months_between(\"visit_month\",\"cohort_month\") as month_diff,\n    count(distinct cohort.userid) as unique_users\nFROM (\n    SELECT userid, MIN(DATE_FORMAT(ts,\u0027yyyy-MM\u0027)) as cohort_month\n    FROM usc\n    JOIN st ON st.sessionid \u003d usc.sessionid\n    GROUP BY 1\n) as cohort \nLEFT JOIN (\n    SELECT distinct userid, DATE_FORMAT(ts,\u0027yyyy-MM\u0027) as visit_month\n    FROM usc\n    JOIN st ON st.sessionid \u003d usc.sessionid    \n) as visit\nON cohort.cohort_month \u003c\u003d visit.visit_month AND cohort.userid \u003d visit.userid\nGROUP BY 1,2\nORDER BY 1,2\n\"\"\").cache()\n# cohorts.show()\ncohorts.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": "%%sql\n"
    }
  ]
}