{
  "paragraphs": [
    {
      "text": "%jdbc\n\nSELECT * FROM ",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 14:58:09.094",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615388277679_991986679",
      "id": "paragraph_1615388277679_991986679",
      "dateCreated": "2021-03-10 14:57:57.688",
      "status": "READY"
    },
    {
      "text": "%pyspark\nimport json\nraw \u003d \u0027[{\"name\": \"grab\"}, {\"name\": \"graby\"}, {\"name\": \"new grab\"}]\u0027\n\nprint(json.loads(raw))",
      "user": "anonymous",
      "dateUpdated": "2021-03-11 07:04:20.410",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[{\u0027name\u0027: \u0027grab\u0027}, {\u0027name\u0027: \u0027graby\u0027}, {\u0027name\u0027: \u0027new grab\u0027}]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040532_100933967",
      "id": "20210310-110040_636004075",
      "dateCreated": "2021-03-10 11:00:40.532",
      "dateStarted": "2021-03-11 07:04:20.436",
      "dateFinished": "2021-03-11 07:04:20.798",
      "status": "FINISHED"
    },
    {
      "text": "%sh\n#User Analytics Data 다운로드\nwget --no-check-certificate -r \u0027https://docs.google.com/uc?export\u003ddownload\u0026id\u003d1MdN9jWZfiw1gFIfCzJlm8t0b2ooBYLSP\u0027 -O raw_data.zip\nunzip raw_data.zip\nhead /zeppelin/session_timestamp.csv",
      "user": "anonymous",
      "dateUpdated": "2021-03-11 06:53:43.103",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/sh",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040532_1756016498",
      "id": "20210310-110040_196701070",
      "dateCreated": "2021-03-10 11:00:40.532",
      "status": "READY"
    },
    {
      "text": "%pyspark\n#df로 import하기\ndf_session_ts \u003d spark.read.option(\"header\", \"true\").csv(\u0027/raw_data/session_timestamp.csv\u0027)\ndf_session_tx \u003d spark.read.option(\"header\", \"true\").csv(\u0027/raw_data/session_transaction.csv\u0027)\ndf_channel \u003d spark.read.option(\"header\", \"true\").csv(\u0027/raw_data/channel.csv\u0027)\ndf_usc \u003d spark.read.option(\"header\", \"true\").csv(\u0027/raw_data/user_session_channel.csv\u0027)\n\n\n# 전체 dataframe 출력\n# df_session_ts.show()|\n\n# explain으로 실행계획 확인하기\n# df_session_ts.select(\"ts\").explain()\n\ndf_usc.limit(1).show()\ndf_session_ts.limit(1).show()\ndf_session_tx.limit(1).show()\ndf_channel.limit(1).show()\n\n#SQL에 사용할 TempView를 만들기\ndf_usc.createOrReplaceTempView(\"usc\")\ndf_session_ts.createOrReplaceTempView(\"st\")\ndf_session_tx.createOrReplaceTempView(\"tx\")\ndf_channel.createOrReplaceTempView(\"ch\")\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 14:54:22.446",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m~opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~opt/zeppelin/interpreter/spark/pyspark/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o57.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/raw_data/session_timestamp.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-8-148a136183de\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#df로 import하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mdf_session_ts\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027/raw_data/session_timestamp.csv\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_session_tx\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027/raw_data/session_transaction.csv\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_channel\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027/raw_data/channel.csv\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_usc\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027/raw_data/user_session_channel.csv\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u003d\u003d\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~opt/zeppelin/interpreter/spark/pyspark/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value \u003d get_return_value(\n\u001b[0;32m-\u003e 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~opt/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027org.apache.spark.sql.AnalysisException: \u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027: \u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027org.apache.spark.sql.catalyst.analysis\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027: \u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: \u0027Path does not exist: file:/raw_data/session_timestamp.csv;\u0027"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040532_1196399538",
      "id": "20210310-110040_1075127419",
      "dateCreated": "2021-03-10 11:00:40.532",
      "dateStarted": "2021-03-10 14:54:22.481",
      "dateFinished": "2021-03-10 14:54:29.219",
      "status": "ERROR"
    },
    {
      "text": "%python\n%pyspark\n#1. 특정 컬럼만 출력하기\n# df_session_ts.select(\u0027sessionid\u0027).show()\n\n# from pyspark.sql.functions import max\n# df_session_tx.select(max(\"amount\")).take(1)\n\n#2. 특정 Row만 필터링하기\n# df_session_ts.filter(df[\u0027ts\u0027] \u003c \u00272019-05-01 01:00:00\u0027).show()\n\n#3. groupby하기 \n# df_session_ts.groupby(\"sessionid\").agg({\u0027ts\u0027: \u0027count\u0027}).show()\n\n#4. sort 해서 2개만 가져오기\n# desc하려면 모듈 import 필요\n# from pyspark.sql.functions import desc\n\n# df_session_ts.sort(\u0027ts\u0027).take(2)\n# df_session_ts.sort(desc(\u0027ts\u0027)).take(2)\n\n\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.533",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040532_532968092",
      "id": "20210310-110040_990409504",
      "dateCreated": "2021-03-10 11:00:40.533",
      "status": "READY"
    },
    {
      "text": "%python\n%\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.533",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040533_1121508494",
      "id": "20210310-110040_1538741588",
      "dateCreated": "2021-03-10 11:00:40.533",
      "status": "READY"
    },
    {
      "text": "%python\n%pyspark\n# Monthly Active User 구하기\n\n# df_usc.show()\n# df_session_ts.show()\nfrom pyspark.sql.functions import count\ndf_session_ts.alias(\"st\").join(df_usc.alias(\"usc\"), df_session_ts.sessionid \u003d\u003d df_usc.sessionid, how\u003d\u0027left\u0027).groupby(\u0027st.ts\u0027).agg(count(\u0027usc.userid\u0027)).show()\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.533",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040533_1088070870",
      "id": "20210310-110040_462594231",
      "dateCreated": "2021-03-10 11:00:40.533",
      "status": "READY"
    },
    {
      "text": "\n%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.533",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040533_277073641",
      "id": "20210310-110040_1917679938",
      "dateCreated": "2021-03-10 11:00:40.533",
      "status": "READY"
    },
    {
      "text": "%python\nSQL로 Growth 지표 뽑아보기",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.533",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040533_518209312",
      "id": "20210310-110040_178558296",
      "dateCreated": "2021-03-10 11:00:40.533",
      "status": "READY"
    },
    {
      "text": "%python\n%pyspark\n\n# MAU 구하기\nspark.sql(\"\"\"\nSELECT date_format(st.ts, \"yyyy-MM\") as month , count(usc.userid)\nFROM usc \nLEFT JOIN st on usc.sessionid \u003d st.sessionid \nGROUP BY 1\nORDER BY 1\n\"\"\").show()\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.534",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040533_169111554",
      "id": "20210310-110040_1800961197",
      "dateCreated": "2021-03-10 11:00:40.533",
      "status": "READY"
    },
    {
      "text": "%python\n%pyspark\n\n# Net Revenue가 가장 큰 UserId 10개 찾기\nspark.sql(\"\"\"\nSELECT usc.userid as uid, SUM(IF(tx.refunded \u003d false, tx.amount, 0)) as total\nFROM tx\nLEFT JOIN usc\nON tx.sessionid \u003d usc.sessionid\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 10\n\"\"\").show()\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.534",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040534_1049821606",
      "id": "20210310-110040_1707208297",
      "dateCreated": "2021-03-10 11:00:40.534",
      "status": "READY"
    },
    {
      "text": "%python\n%pyspark\n#채녈 별 사용자 구하기\nspark.sql(\"\"\"\nSELECT ch.channelname, COUNT(usc.userid) as count\nFROM ch\nLEFT JOIN usc\nON ch.channelname \u003d usc.channel\nGROUP BY ch.channelname\nORDER BY 2 desc\n\"\"\").show()\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.534",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040534_378176665",
      "id": "20210310-110040_2086608122",
      "dateCreated": "2021-03-10 11:00:40.534",
      "status": "READY"
    },
    {
      "text": "%python\n\n%sql\n\nSELECT distinct userid, FIRST_VALUE(channel) over (partition by userid order by ts rows between unbounded preceding and unbounded following) as first, LAST_VALUE(channel) over (partition by userid order by ts rows between unbounded preceding and unbounded following) as last\nFROM usc\nLEFT JOIN st\nON st.sessionid \u003d usc.sessionid\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.534",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040534_959355411",
      "id": "20210310-110040_1384077436",
      "dateCreated": "2021-03-10 11:00:40.534",
      "status": "READY"
    },
    {
      "text": "%python\n%sql\nSELECT \n    DATE_FORMAT(st.ts,\"YYYY-MM\") as month, \n    channel, \n    count(distinct usc.userid) as uniqueUsers,\n    count(amount) as paidUsers,\n    CONCAT(IF(count(amount) \u003d 0, 0,round((count(amount) / count(distinct usc.userid)) * 100,1)),\u0027%\u0027) as conversionRate,\n    SUM(amount) as growthRevenue,\n    SUM(IF(refunded \u003d FALSE ,amount, 0)) as netRevenue\nFROM ch\nLEFT JOIN usc\non ch.channelname \u003d usc.channel\nLEFT JOIN st\non st.sessionid \u003d usc.sessionid\nLEFT JOIN tx\non tx.sessionid \u003d usc.sessionid\nWHERE channel IS NOT NULL\nGROUP BY 1, 2\nORDER BY 1, 2\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.534",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040534_978410083",
      "id": "20210310-110040_2062216074",
      "dateCreated": "2021-03-10 11:00:40.534",
      "status": "READY"
    },
    {
      "text": "%python\n%pyspark\nfrom pyspark.sql.functions import months_between, countDistinct, col\n\ncohorts \u003d spark.sql(\"\"\"\nSELECT \n    cohort_month,\n    months_between(\"visit_month\",\"cohort_month\") as month_diff,\n    count(distinct cohort.userid) as unique_users\nFROM (\n    SELECT userid, MIN(DATE_FORMAT(ts,\u0027yyyy-MM\u0027)) as cohort_month\n    FROM usc\n    JOIN st ON st.sessionid \u003d usc.sessionid\n    GROUP BY 1\n) as cohort \nLEFT JOIN (\n    SELECT distinct userid, DATE_FORMAT(ts,\u0027yyyy-MM\u0027) as visit_month\n    FROM usc\n    JOIN st ON st.sessionid \u003d usc.sessionid    \n) as visit\nON cohort.cohort_month \u003c\u003d visit.visit_month AND cohort.userid \u003d visit.userid\nGROUP BY 1,2\nORDER BY 1,2\n\"\"\").cache()\n# cohorts.show()\ncohorts.show()",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.534",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040534_2122163584",
      "id": "20210310-110040_280158232",
      "dateCreated": "2021-03-10 11:00:40.534",
      "status": "READY"
    },
    {
      "text": "%python\n%%sql\n",
      "user": "anonymous",
      "dateUpdated": "2021-03-10 11:00:40.534",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1615374040534_1940539032",
      "id": "20210310-110040_336500280",
      "dateCreated": "2021-03-10 11:00:40.534",
      "status": "READY"
    }
  ],
  "name": "session-analyze",
  "id": "2G2SZXUX5",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}